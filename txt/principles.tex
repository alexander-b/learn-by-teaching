We wish to discuss \textit{learning} itself, and how this knowledge affects 
our system. A common technique used in most of these systems is \textbf{spaced 
repetition} combined with \textbf{testing}, i.e.\ the pacing of repetition 
throughout time. This approach is well-proven by research, and could be 
implemented both as a part of our services (allowing module authors to apply 
it to their own students), or as a part of the experience of learning the 
tools. An example might be: `After 5 days, please repeat this factoid and test 
the student.` Of course, such an approach might prove cumbersome, and 
techniques like machine learning might automate it in some meaningful fashion 
using a data-oriented approached. Reminders are most effective when occurring 
just before the memory fades. Successive reminders will consequently be 
separated at longer and longer intervals\cite{memrise}.

In testing, there exists mainly three techniques for assessment:

\begin{itemize}
\item Reading, which is the reproduction of textual representations of the fact.
\item Multiple choice, where one is allowed multiple alternatives, where at
least one of them should be true. Often one or more of the alternatives are `the
odd one out', to test the student's ability to discriminate irrelevant 
concepts.
\item Generation, where the user is made to produce something, for example
filling in a word. This technique is highly efficient with regards to 
retention, especially when combined with quick 
feedback\cite{potts2014benefit}.
\end{itemize}

Thus, we consider generation. An example of this might be generating a formally
valid blueprint of some e-module, and asking the user what it does. Or it might
be to make the user connect the relevant parts to make it work as desired. Thus,
we foster true comprehension of the system mechanics and avoid cramming. As
noted in the literature in the Memrise review, errorful generation might 
actually be beneficial, and the user should not be punished for making 
mistakes. Rather, we suggest to embrace it and make failure as smooth as 
possible, with quick iterations.

A big part of any learning system is the \textit{metaphor}, or the user
interface. The system needs to provide some incentive for the user to perform
some action. Thus, the user interface will always be a question of demographics.
We suggest that the user interface should be amenable to at least some
simplifications to scale down to the technology-native elementary school
demographic. Some concrete suggestions are:

\begin{itemize}
\item The user interface needs to take into consideration the rather poor motor
skills of young children\cite{kidsquora}. Thus, tap actions should not be that
precise or responsive, but instead allow for some slack. For example, multiple
taps should not result in multiple windows, but rather provide a time buffer for
the action to launch. Also, the hit box of buttons should be greater than their
actual size.
\item Avoid splash screens\cite{kidsluke}, and in general get to the point as 
quickly as
possible. Thus, it might be better to provide a simpler, but usable interface
rather than a complete experience first-hand. E.g.\ one may wish to avoid 
modal dialogues, as they provide children which textual choice, which they
might not understand or be interested in.
\item To provide some sense of continuity, and also a way of accumulating
rewards, the user constructs an avatar. This concept is known from video
games, and has already been suggested to be a viable way of familiarising
children with complicated services like search engines\cite{gossen2012search}.
\end{itemize}

Unfortunately, we were not able to locate high-quality sources of research in
this area. Most sources are based on de-facto and ad-hoc solutions, with little
or no scientific basis outside of in-house usability studies. Thus, we are
potentially missing out on a great deal of useful data. On the other hand, 
this means we have the unique opportunity to not only be innovators, but also 
inspiring flagships.

\textbf{Takeaway points}, based on previous discussion and the reviews in 
Section~\ref{related}.

\begin{itemize}
\item Spaced learning with testing using generation fosters comprehension and
recall. We need to embrace generating errors as a way of learning.
\item Quick and simple feedback with additive gains facilitate user engagement
and learning greatly, and promotes attention continuity.
\item The system should provide a simple (preferably physical) metaphor as part
of its user story.
\item The system needs to use open, free standards to enhance composition and
foster a culture of reuse and experimentation.
\item We should strive for feature parity between different clients, to simplify
the user story and provide a consistent experience.
\item A strong, data-driven approach involving the users strengthens community
relations and improved quality by providing relevant feedback to create,
enhance, or QA existing material.
\end{itemize}

So the big question then becomes what do we do with all these takeaway points. 
In order to provide a cohesive experience for authors as well as users we need 
to take all of this into account. But cohesion traditionally proves difficult 
when coupled with agnosticism --- i.e.\ we would like to compose many --- 
different --- kinds of modules, and different kinds of modules likely have 
different approaches to authoring.

What we are eventually getting at is the need for high quality control in what 
modules we allow, and likely a fair bit of customisation of the ones we do end 
up allowing. Alternatively new module authoring software may be developed 
altogether.
