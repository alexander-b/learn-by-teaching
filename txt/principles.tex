As part of the research, reviews of related services offering learning
experiences were written. The individual findings can be found in Section
~\ref{ch:related} on related work (found on page~\pageref{ch:related}). In this
Section, we try to apply the findings towards our end of producing a system fit
for students and educators alike.

A general principle in most of these systems seems to be \textbf{spacing}
combined with \textbf{testing}, that is the pacing of repetition throughout time
to increase retention and foster comprehension of the material at hand. As
provided in the Memrise review, most research on the topic supports this
technique.

What, then, would such a technique mean for our e-module author system? On one
hand, it might an option to allow authors themselves to introduce spacing into
their lessons. For example, allow for `after 5 days, please repeat this message
and test the student`. On the other hand, such manual spacing might provide
cumbersome and boring to most users, and should probably be provided by the
system itself using some heuristics.

Furthermore: Spacing might provide invaluable to teach the e-module authoring
tools themselves. For example, the user might unlock a feature, and then be
reminded in some day (say, five days) that the feature is still available and
could be used to some effect in the current project.

This however leaves us with a remaining question: What should such testing look
like? One example might be to provide randomly generated boards with features,
and ask the user to explain what action would be performed if this module was to
be run. This could for example be accomplished using the familiar multiple
choice form of testing. However, as \cite{potts2014benefit} and others have
shown, this would surely be less than some form of \textbf{generation}, that is
making the user produce something themselves. A solution might then be to
generate some random \textit{blueprint} for a module, make the user construct it
(not unlike playing LEGO), and then guessing the outcome of running the finished
module. This would provide generation in a form that might help the user
learning the system, and might also prevent cramming, which might hinder
learning by fostering mere reproduction instead of true comprehension of the
learning engine mechanics.

\textbf{We summarise the takeaway points given in the reviews:}

\begin{itemize}
\item Spaced learning with testing using generation fosters comprehension and
recall. We need to embrace generating errors as a way of learning.
\item Quick and simple feedback with additive gains facilitate user engagement
and learning greatly, and promotes attention continuity.
\item The system should provide a simple (preferably physical) metaphor as part
of its user story.
\item The system needs to use open, free standards to enhance composition and
foster a culture of reuse and experimentation.
\item We should strive for feature parity between different clients, to simplify
the user story and provide a consistent experience.
\item A strong, data-driven approach involving the users strengthens community
relations and improved quality by providing relevant feedback to create,
enhance, or QA existing material.
\end{itemize}
